#FORMATO PYTHON

#"MAIN" FORMATEO DE DATOS A CSV



import pandas as pd

# Definicion de la ruta del archivo con los datos a evaluar
df = pd.read_excel(r"C:\Users\nicos\Python Projects\Volumen Exportacion.xlsx", sheet_name="Datos")

#Imprimir los datos por pantalla
#print(df)
#print(df['Date Time'].dtype)

# Se define la fecha y hora como indice de los datos
df['datetime'] = pd.to_datetime(
    df['Date Time'],
    format = '%d-%m-%Y'
)
df = df.set_index('datetime')
df.sort_index(inplace=True)
df = df.drop(columns=['Date Time'])


#Conteo de datos faltantes por columna
print('Cantidad de NaNs:')
for column in df:
    nans = df[column].isna().sum()
    print(f'\tColumna {column}: {nans}')

#Diferencia de tiempo entre datos
df_time_diffs = df.index.to_series().diff().dt.total_seconds()
print(df_time_diffs.value_counts())

#Se rellenan los datos faltantes con 0
df2 = df.asfreq(freq='D', fill_value=0)
df_time_diffs = df2.index.to_series().diff().dt.total_seconds()
print(df_time_diffs.value_counts())

#Se exportan los datos como archivo csv
df2.to_csv(r"C:\Users\nicos\Python Projects\Datos Volumen Procesados.csv")









#FORMATEO DE DATOS, DEFINICION DE MODELO, ENTRENAMIENTO DEL MODELO Y RESULTADOS

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

#Se importan los datos en formato csv
df = pd.read_csv(r"C:\Users\nicos\Python Projects\Datos Volumen Procesados.csv")


def train_val_test_split(serie, tr_size=0.8, vl_size=0.1, ts_size=0.1 ):
    # Definición del número de datos en cada subserie
    N = serie.shape[0]
    Ntrain = int(tr_size*N)  # Número de datos de entrenamiento
    Nval = int(vl_size*N)    # Número de datos de validación
    Ntst = N - Ntrain - Nval # Número de datos de prueba

    # Realizar partición
    train = serie[0:Ntrain]
    val = serie[Ntrain:Ntrain+Nval]
    test = serie[Ntrain+Nval:]

    return train, val, test


tr, vl, ts = train_val_test_split(df['Volumen [TEU]'])

# Imprimir en pantalla el tamaño de cada subset
print(f'Tamaño set de entrenamiento: {tr.shape}')
print(f'Tamaño set de validación: {vl.shape}')
print(f'Tamaño set de prueba: {ts.shape}')


def crear_dataset_supervisado(array, input_length, output_length):
    # Inicialización
    X, Y = [], []  # Listados que contendrán los datos de entrada y salida del modelo
    shape = array.shape
    if len(shape) == 1:  # Para cuando el modelo solo usa una variable
        fils, cols = array.shape[0], 1
        array = array.reshape(fils, cols)
    else:  # Se utiliza en caso de que sea mas de una variable
        fils, cols = array.shape

    # Generar los arreglos
    for i in range(fils - input_length - output_length):
        X.append(array[i:i+INPUT_LENGTH,0:cols])
        Y.append(array[i + input_length:i + input_length + output_length, -1].reshape(output_length, 1))

    # Convertir listas a arreglos de NumPy
    X = np.array(X)
    Y = np.array(Y)

    return X, Y


# Definición de los hiperparámetros INPUT_LENGTH y OUTPUT_LENGTH
INPUT_LENGTH = 7    # Cuantos periodos de entrenamiento
OUTPUT_LENGTH = 1    # Cuantos periodos seran predecidos

# Datasets supervisados para entrenamiento (x_tr, y_tr), validación
# (x_vl, y_vl) y prueba (x_ts, y_ts)
x_tr, y_tr = crear_dataset_supervisado(tr.values, INPUT_LENGTH, OUTPUT_LENGTH)
x_vl, y_vl = crear_dataset_supervisado(vl.values, INPUT_LENGTH, OUTPUT_LENGTH)
x_ts, y_ts = crear_dataset_supervisado(ts.values, INPUT_LENGTH, OUTPUT_LENGTH)

#Imprimir por pantalla los tamaños de los sets
print('Tamaños entrada (BATCHES x INPUT_LENGTH x FEATURES) y de salida (BATCHES x OUTPUT_LENGTH x FEATURES)')
print(f'Set de entrenamiento - x_tr: {x_tr.shape}, y_tr: {y_tr.shape}')
print(f'Set de validación - x_vl: {x_vl.shape}, y_vl: {y_vl.shape}')
print(f'Set de prueba - x_ts: {x_ts.shape}, y_ts: {y_ts.shape}')

from sklearn.preprocessing import MinMaxScaler

#Se convierte los datos del set a valores entre -1 y 1
def escalar_dataset(data_input):
    NFEATS = data_input['x_tr'].shape[2]

    # Generar listado con "scalers"
    scalers = [MinMaxScaler(feature_range=(-1, 1)) for i in range(NFEATS)]

    # Arreglos que contendrán los datasets escalados
    x_tr_s = np.zeros(data_input['x_tr'].shape)
    x_vl_s = np.zeros(data_input['x_vl'].shape)
    x_ts_s = np.zeros(data_input['x_ts'].shape)
    y_tr_s = np.zeros(data_input['y_tr'].shape)
    y_vl_s = np.zeros(data_input['y_vl'].shape)
    y_ts_s = np.zeros(data_input['y_ts'].shape)

    # Escalamiento: se usarán los min/max del set de entrenamiento para escalar la totalidad de los datasets

    # Escalamiento Xs
    for i in range(NFEATS):
        x_tr_s[:, :, i] = scalers[i].fit_transform(x_tr[:, :, i])
        x_vl_s[:, :, i] = scalers[i].transform(x_vl[:, :, i])
        x_ts_s[:, :, i] = scalers[i].transform(x_ts[:, :, i])

    # Escalamiento Ys
    y_tr_s[:, :, 0] = scalers[-1].fit_transform(y_tr[:, :, 0])
    y_vl_s[:, :, 0] = scalers[-1].transform(y_vl[:, :, 0])
    y_ts_s[:, :, 0] = scalers[-1].transform(y_ts[:, :, 0])

    # Conformar datasets de salida
    data_scaled = {
        'x_tr_s': x_tr_s, 'y_tr_s': y_tr_s,
        'x_vl_s': x_vl_s, 'y_vl_s': y_vl_s,
        'x_ts_s': x_ts_s, 'y_ts_s': y_ts_s,
    }

    return data_scaled, scalers[0]

data_in = {
    'x_tr': x_tr, 'y_tr': y_tr,
    'x_vl': x_vl, 'y_vl': y_vl,
    'x_ts': x_ts, 'y_ts': y_ts,
}

#Se aplica el metodo a los datos
data_s, scaler = escalar_dataset(data_in)

x_tr_s, y_tr_s = data_s['x_tr_s'], data_s['y_tr_s']
x_vl_s, y_vl_s = data_s['x_vl_s'], data_s['y_vl_s']
x_ts_s, y_ts_s = data_s['x_ts_s'], data_s['y_ts_s']

from tensorflow.python.keras.models import Sequential
from tensorflow.python.keras.layers import LSTM, Dense
from tensorflow.python.keras.optimizers import rmsprop_v2

import tensorflow as tf

tf.random.set_seed(123)
tf.config.experimental.enable_op_determinism()

#Se define el numero de neuronas para el modelo
N_UNITS = 128

INPUT_SHAPE = (x_tr_s.shape[1], x_tr_s.shape[2])

#Se define el modelo
modelo = Sequential()
modelo.add(LSTM(N_UNITS, input_shape=INPUT_SHAPE))
modelo.add(Dense(OUTPUT_LENGTH, activation='linear'))

#Metodo para evaluar el RSME
def root_mean_squared_error(y_true, y_pred):
    rmse = tf.math.sqrt(tf.math.reduce_mean(tf.square(y_pred-y_true)))
    return rmse

#Se define el optimizador para el modelo
optimizador = rmsprop_v2.RMSprop(learning_rate=0.0005)

modelo.compile(
    optimizer = optimizador,
    loss = root_mean_squared_error,
)

EPOCHS = 100 #Cantidad de entrenamientos
BATCH_SIZE = 100 #Cantidad de datosa usar en cada entrenamiento

#Almacenamiento de los datos de entrenamiento y validacion
historia = modelo.fit(
    x = x_tr_s,
    y = y_tr_s,
    batch_size = BATCH_SIZE,
    epochs = EPOCHS,
    validation_data = (x_vl_s, y_vl_s),
    verbose=2
)

rmse_tr = modelo.evaluate(x=x_tr_s, y=y_tr_s, verbose=0)
rmse_vl = modelo.evaluate(x=x_vl_s, y=y_vl_s, verbose=0)
rmse_ts = modelo.evaluate(x=x_ts_s, y=y_ts_s, verbose=0)

#Se imprime por pantalla los RMSE de cada set
print('Comparativo desempeños:')
print(f'  RMSE train:\t {rmse_tr:.3f}')
print(f'  RMSE val:\t {rmse_vl:.3f}')
print(f'  RMSE test:\t {rmse_ts:.3f}')

#Se imprime por pantalla la perdida de valor con cada iteración
plt.plot(historia.history['loss'], label='RMS train')
plt.plot(historia.history['val_loss'], label='RMS val')

plt.xlabel('Iteración')
plt.ylabel('RMSE')
plt.legend()

plt.show()


def predecir(x, model, scaler):
    # Calcular predicción escalada en el rango de -1 a 1
    y_pred_s = model.predict(x,verbose=0)

    # Llevar la predicción a la escala original
    y_pred = scaler.inverse_transform(y_pred_s)

    return y_pred.flatten()

y_ts_pred = predecir(x_ts_s, modelo, scaler)

N = len(y_ts_pred)    # Número de predicciones (tamaño del set de prueba)
ndato = np.linspace(1,N,N)

# Cálculo de errores simples
errores = abs(y_ts.flatten()-y_ts_pred)/y_ts_pred
y_ts_predplt.plot(errores)
plt.show()